{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4eCr83d1V2Kr",
        "outputId": "40e362ff-0557-4c8e-c173-a28c0427fe32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0, Cost: 0.6931471805599453\n",
            "Iteration 100, Cost: 0.3091972820659265\n",
            "Iteration 200, Cost: 0.30440004733873793\n",
            "Iteration 300, Cost: 0.30317359038006847\n",
            "Iteration 400, Cost: 0.30233536541770467\n",
            "Iteration 500, Cost: 0.30160267998547524\n",
            "Iteration 600, Cost: 0.30093642688194117\n",
            "Iteration 700, Cost: 0.30032573130154755\n",
            "Iteration 800, Cost: 0.29976386512216635\n",
            "Iteration 900, Cost: 0.29924522692574224\n",
            "Iteration 1000, Cost: 0.2987649023645948\n",
            "Iteration 1100, Cost: 0.298318552592667\n",
            "Iteration 1200, Cost: 0.2979023476860833\n",
            "Iteration 1300, Cost: 0.29751290990436313\n",
            "Iteration 1400, Cost: 0.2971472625975246\n",
            "Iteration 1500, Cost: 0.29680278415677785\n",
            "Iteration 1600, Cost: 0.2964771667048008\n",
            "Iteration 1700, Cost: 0.2961683791939589\n",
            "Iteration 1800, Cost: 0.29587463455454494\n",
            "Iteration 1900, Cost: 0.29559436053004173\n",
            "Train Accuracy: 87.58%\n",
            "Test Accuracy: 88.13%\n",
            "\n",
            "Classification Report (Test Data):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.92      0.92      7509\n",
            "           1       0.76      0.77      0.76      2491\n",
            "\n",
            "    accuracy                           0.88     10000\n",
            "   macro avg       0.84      0.84      0.84     10000\n",
            "weighted avg       0.88      0.88      0.88     10000\n",
            "\n",
            "\n",
            "Confusion Matrix (Test Data):\n",
            "[[6907  602]\n",
            " [ 585 1906]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Step 1: Initialize parameters\n",
        "def initialize_parameters(num_features):\n",
        "    weights = np.zeros((num_features, 1))\n",
        "    bias = 0\n",
        "    return weights, bias\n",
        "\n",
        "# Step 2: Sigmoid function\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "# Step 3: Compute predictions\n",
        "def compute_predictions(X, weights, bias):\n",
        "    z = np.dot(X, weights) + bias\n",
        "    y_pred = sigmoid(z)\n",
        "    return y_pred\n",
        "\n",
        "# Step 4: Compute cost\n",
        "def compute_cost(y_true, y_pred, epsilon=1e-15):\n",
        "    \"\"\"\n",
        "    Compute the log loss (cost) for logistic regression.\n",
        "\n",
        "    Parameters:\n",
        "    - y_true: True labels (0 or 1)\n",
        "    - y_pred: Predicted probabilities (between 0 and 1)\n",
        "    - epsilon: Small value to avoid log(0) or log(1)\n",
        "\n",
        "    Returns:\n",
        "    - cost: Computed log loss\n",
        "    \"\"\"\n",
        "    # Clip predictions to avoid log(0) or log(1)\n",
        "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
        "\n",
        "    # Compute the cost\n",
        "    m = y_true.shape[0]  # Number of samples\n",
        "    cost = -(1/m) * np.sum(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
        "\n",
        "    return cost\n",
        "\n",
        "# Step 5: Gradient descent\n",
        "def gradient_descent(X, y_true, y_pred, weights, bias, learning_rate):\n",
        "    m = y_true.shape[0]\n",
        "    dw = (1/m) * np.dot(X.T, (y_pred - y_true))\n",
        "    db = (1/m) * np.sum(y_pred - y_true)\n",
        "    weights -= learning_rate * dw\n",
        "    bias -= learning_rate * db\n",
        "    return weights, bias\n",
        "\n",
        "# Step 6: Train the model\n",
        "def train_logistic_regression(X_train, y_train, learning_rate=0.01, num_iterations=1000):\n",
        "    num_features = X_train.shape[1]\n",
        "    weights, bias = initialize_parameters(num_features)\n",
        "    y_train = y_train.values.reshape(-1, 1)\n",
        "\n",
        "    for i in range(num_iterations):\n",
        "        y_pred = compute_predictions(X_train, weights, bias)\n",
        "        cost = compute_cost(y_train, y_pred)\n",
        "        weights, bias = gradient_descent(X_train, y_train, y_pred, weights, bias, learning_rate)\n",
        "        if i % 100 == 0:\n",
        "            print(f\"Iteration {i}, Cost: {cost}\")\n",
        "\n",
        "    return weights, bias\n",
        "\n",
        "# Step 7: Make predictions\n",
        "def predict(X, weights, bias):\n",
        "    y_pred = compute_predictions(X, weights, bias)\n",
        "    y_pred_class = (y_pred > 0.45).astype(int)\n",
        "    return y_pred_class\n",
        "\n",
        "# Load data and split into features and target\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Dataset/filtered_climate_data_final.csv\");\n",
        "X = df.drop(columns = [\"isRainy\",\"MinTemp_2m\",\"TempRange_2m\",\"MaxWindSpeed_10m\",\"MinWindSpeed_10m\",\"WindSpeedRange_10m\",\"WindSpeed_50m\",\"MaxWindSpeed_50m\",\"MinWindSpeed_50m\",\"Precip\",\"WindSpeedRange_50m\"]); #dropping unnecessary features\n",
        "\n",
        "y = df[\"isRainy\"]\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "weights, bias = train_logistic_regression(X_train, y_train, learning_rate=0.001, num_iterations=2000)\n",
        "\n",
        "# # Make predictions\n",
        "# y_pred_test = predict(X_test, weights, bias)\n",
        "\n",
        "# # Evaluate the model\n",
        "# accuracy = accuracy_score(y_test, y_pred_test)\n",
        "\n",
        "# Train accuracy\n",
        "y_pred_train = predict(X_train, weights, bias)  # Predict on training set\n",
        "train_accuracy = accuracy_score(y_train, y_pred_train) * 100\n",
        "\n",
        "# Test accuracy\n",
        "y_pred_test = predict(X_test, weights, bias)  # Predict on test set\n",
        "test_accuracy = accuracy_score(y_test, y_pred_test) * 100\n",
        "\n",
        "# Print results\n",
        "print(f\"Train Accuracy: {train_accuracy:.2f}%\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "print(\"\\nClassification Report (Test Data):\")\n",
        "print(classification_report(y_test, y_pred_test))\n",
        "\n",
        "print(\"\\nConfusion Matrix (Test Data):\")\n",
        "print(confusion_matrix(y_test, y_pred_test))\n"
      ]
    }
  ]
}